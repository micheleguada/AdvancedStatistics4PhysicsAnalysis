{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://avatars3.githubusercontent.com/u/43672704?s=400&u=7f10d18e6375065a2bd501c9cfd59a2ac6ad0f80&v=4\">\n",
    "\n",
    "# Advanced Statistics For Physics Analysis\n",
    "## Bayesian Blocks: an algorithm for histogram representation\n",
    "**Authors:**\n",
    "* Alessandro Lambertini - ID: 1242885\n",
    "* Michele Guadagnini    - ID: 1230663\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook:\n",
    "- We implement the Bayesian Block algorithm in R language.\n",
    "- We test the algorithm functioning under different assumptions with different datasets ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief theoretical introduction:\n",
    "\n",
    "### Goal to achieve:\n",
    "***Bayesian Blocks*** is a *non-parametric* analysis method for sequential data whose goal is to represent a signal with an optimal set of data blocks that allows to deal with measurements errors and noise while highlighting its important features and local structure. \n",
    "It is built in such a way that it imposes as few preconditions and assumptions as possible in the signal shape, scale and resolution. The algorithm is born in 1998 and improved in 2013 by Scargle in the context of astronomical time series analysis.\n",
    "\n",
    "### The Piecewise Constant Model:\n",
    "The objective of this analysis is to divide the range of the independent variable into subintervals, called **blocks**, in which the dependent variable (e.g. counts of events, amplitude of a signal ...) can be modeled as *constant*. The algorithm aims at providing the **best partition** of the independent variable by **maximizing** a certain goodness-of-fit measure that is called **fitness**. \n",
    "Using a *fitness* function that is *block-additive*, meaning that the total fitness of the partition is the sum of the fitness values of all the blocks, allows to treat the blocks independently, in the sense that a block’s fitness depends only on its own data.\n",
    "The block-additivity property can be expressed with the following equation:\n",
    "\n",
    "<center> $F[P(T)] = \\sum_{k=1}^{N_{blocks}} f(B_{k})$, </center>\n",
    "\n",
    "where $F[P(T)]$ is the total fitness of the partition $P$ of interval $T$ and $f(B_k)$ is the fitness of block *k*, a measure of how well a constant signal represents the data within it.\n",
    "\n",
    "The time at which the signal presents an abrupt transition in its amplitude is called **change point**. The algorithm is meant to detect these change points and use them as edges of data blocks.\n",
    "As a non-parametric analysis technique, finding the optimal partition involves controlling in some way the complexity of the estimated representation, namely the number of blocks. In a usual application the number of blocks is expected to be much smaller than the number of data analysed. It is possible to influence the number of blocks by defining a prior distribution. For example it can be used a geometric prior with the single parameter $\\gamma$:\n",
    "\n",
    "<center> $P(N_{blocks}) = P_{0} \\gamma^{N_{blocks}}$ </center>\n",
    "\n",
    "where $P_{0}$ is the desired *'correct detection rate'*. In the usual fashion for Bayesian model selection, in case of high signal-to-noise ratios, $N_{blocks}$ is determined by the structure of the signal, while, with lower signal-to-noise, the prior becomes more and more important.\n",
    "\n",
    "### The algorithm\n",
    "It is convinient to represent the input data of the algorithm with **data cells**. Data cells are defined with two fundamental features: *cell width* and its *content*. In this context a block is any set of consecutive cells and a partition is simply a collections of non-overlapping blocks. \n",
    "The number of possible partitions is $2^{N-1}$, with $N$ representing the number of data cells, so an exhaustive search for the optimal partition is impossible for any real dataset. Hence, the algorithm follows an iterative procedure similar to mathematical induction: beginning with the first data cell, a new cell is added at each step and the best partition is selected making use of the results of all the previous steps. Indeed, a key concept of the proposed algorithm is expressed in the theorem below: <br>\n",
    "\n",
    "**Theorem**: *Removing the last block of an optimal partition leaves an optimal partition*\n",
    "\n",
    "This fact allows to reduce the computational cost of the algorithm to $O(N^2)$. <br>\n",
    "\n",
    "The description of the core of the algorithm follows: <br>\n",
    "Let $P_{opt}(R)$ denote the optimal partition of the first $R$ cells. In the starting case $R = 1$, the only possible partition is trivially optimal. \n",
    "At step $R+1$ the results of the previous $R$ steps are stored in the arrays **best**, containing the fitness of the old optimal partitions, and **last**, containing the position of the last change point at each iteration. <br>\n",
    "To obtain the optimal partition $P_{opt}(R+1)$, consider the set of all partitions that have the last block starting with the cell $k$ and ending at cell $R+1$. \n",
    "By making use of the *additivity* of the fitness function and the *theorem* reported above, it is possible to compute the overall fitness of the partition $P_{opt}(R+1)$ by summing the fitness of this last block, $f(k)$, with the fitness of the optimal partition obtained in the previous steps:\n",
    "\n",
    "<center>\n",
    "$A(k) = f(k) + \\begin{cases}\n",
    "                 0,         & \\text{if } k = 1 \\\\\n",
    "                 best(k-1), & \\text{if } k = 2 ... R+1\n",
    "               \\end{cases}  $\n",
    "</center>\n",
    "\n",
    "where $A(k)$ contains the fitness of all the partitions $P(R+1)$ that can possibly be optimal. The optimal one is easily found by maximizing $A(k)$. The maximum value is stored in the array *best*, while the value of *k*, index of the maximum, is stored in *last*. \n",
    "\n",
    "At the end of the iterations, when all the data cells has been used, it is necessary to retrieve the change points locations by using the information contained in *last*. Indeed, by using the last value in this array and removing the section corresponding to the last block repeatedly, the change points can be found in reversed order. In symbols, denoting the number of change points with $Ncp$:\n",
    "<br><br>\n",
    "<center> $cp_{Ncp} = last(N) \\\\ cp_{(Ncp-1)} = last(cp_{Ncp} − 1) \\\\  ... \\\\ cp_1 = last(cp_2 -1)$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The implementation\n",
    "\n",
    "The algorithm implementation receives as input the following parameters:\n",
    "- **data_mode** : integer value used to select the operational mode of the algorithm according to the type of input data\n",
    "- **times** : data array containing time-tags or more in general the independent variable\n",
    "- **weights** : data array containing counts of a histogram or measurements of the dependent variable\n",
    "- **sigmas** : data array containing error of measurements (optional)\n",
    "- **gamma** : float value used to compute the prior as: log(*gamma*) (optional)\n",
    "- **p1** : float value representing *false positive rate* used to compute the calibrated prior (optional)\n",
    "\n",
    "**[Lines: 26 - 65]** The first part of the function implements some checks on consistency between *data_mode* value and input data. In particular: <br>\n",
    "- if *data_mode=1*, the function ensures that the array `times` has been provided and that data contained in it are sorted and then, in case it finds repeated values, it puts them together and sums up their weights. <br>\n",
    "- if *data_mode=2* the function ensures that `weights` has been passed and modifies the values that are equal to $0$ by dividing all the weights by their minimum (different from $0$) and adding a small offset ($10^{-4}$) to the zero values. We need to remove zero values because they would rise an error when computing the logarithm in the fitness function and we can do this because, according to [1], the signal amplitude can be treated as a 'nuisance parameter' and we are returning only the change points. <br>\n",
    "- if *data_mode=3* the function checks the presence of the array `weights` and eventually initializes the missing optional arrays. <br>\n",
    "\n",
    "**[Lines: 72 - 99]** This part defines the data cells edges and the **Fitness** and **Prior** functions. *data_mode=1* and *data_mode=2* share the same Fitness and Prior implementations, while *data_mode=3* has its own Fitness and Prior. <br>\n",
    "Note that one can also use a *flat prior* by passing to the algorithm `gamma = 1`.\n",
    "\n",
    "**[Lines: 102 - 130]** Here the algorithm enters the *loop* over data cells, where the fitness is computed according to the selected data mode and the prior contribution is summed to it. Note that the object `fitness_k` is an array that contains the fitness values of all the possible last blocks at iteration `k`. Then, this array is summed with the best optimal partitions obtained in the previous steps up to `k-1`. It is now possible to get the new optimal partition by selecting the maximum value and its index from the array `A_k` and store them into the arrays `best` and `last`, respectively. \n",
    "\n",
    "**[Lines: 132 - 140]** Finally, it is only needed to retrieve the **change points** by iteratively peeling off the last block from the array `last`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayesian_Blocks <- function(data_mode=1, times=0, weights=0, sigmas=1, gamma=0.01, p1=0.01) {\n",
    "    \n",
    "#    Bayesian Blocks algorithm\n",
    "#\n",
    "#    A nonparametric modeling technique that finds the optimal segmentation of the data in the \n",
    "#    observation interval. This function returns a list of optimal change points of a \n",
    "#    one-dimensional time series or sequential data. \n",
    "#    Implementation based on [1^].\n",
    "#    --------------------------------------------------------------------------------------------\n",
    "#    Parameters:\n",
    "#    - data_mode: '1' for Event data, '2' for Binned data, '3' for Points Measurements with \n",
    "#                 known error distribution (numbering chosen to be consistent with [1^])\n",
    "#    - times    : array containing time-tags (or in general the independent variable)\n",
    "#    - weights  : array containing counts (data_mode=2) or measures (data_mode=3)\n",
    "#    - sigmas   : array containing errors of measures (data_mode=3)\n",
    "#    - gamma    : float used to compute Prior as: log(gamma). (ignored if 'p1' is provided \n",
    "#                 or 'data_mode'=3)\n",
    "#    - p1       : float used to compute calibrated Prior as reported in [2^] \n",
    "#    --------------------------------------------------------------------------------------------\n",
    "#    \n",
    "#    [^1]: J. D. Scargle et al., Astrophys. J. 764 (2013) 167, URL: \n",
    "#            https://iopscience.iop.org/article/10.1088/0004-637X/764/2/167\n",
    "#    [^2]: J. D. Scargle et al., *The Bayesian Block Algorithm*, 2013, URL: \n",
    "#            https://arxiv.org/abs/1304.2818\n",
    "\n",
    "### selecting data_mode ###\n",
    "    if (data_mode == 1) { # Event data\n",
    "        \n",
    "        if (missing(times)) stop(\"with data_mode = 1, 'times' must be specified\")\n",
    "        if (!missing(weights)) cat(\"passed weights will be ignored...\",\n",
    "                                   \" If they are meaningful please use data_mode = 2\")         \n",
    "        \n",
    "        # sorting and dealing with repeated values\n",
    "        table   <- rle(sort(times))\n",
    "        times   <- table$values\n",
    "        weights <- table$lengths\n",
    "        \n",
    "    }\n",
    "    else if (data_mode == 2) { # Binned data\n",
    "        \n",
    "        if (missing(weights)) stop(\"with data_mode = 2, 'weights' must be specified\")\n",
    "        if (missing(times  )) { times <- c(1:length(weights)) }\n",
    "\n",
    "        # deal with zero entries that could give error with fitness computation\n",
    "        if (sum(weights)!=0) {\n",
    "            step    <- min(weights[weights!=0])\n",
    "            weights <- weights/step     # normalize weights\n",
    "        }\n",
    "        weights[weights==0] <- 1e-4\n",
    "        \n",
    "        #times   <- times[weights!=0]\n",
    "        #weights <- weights[weights!=0]\n",
    "        \n",
    "    }\n",
    "    else if (data_mode == 3) { # Points Measurements\n",
    "        \n",
    "        if (missing(weights)) stop(\"with data_mode = 3, 'weights' must be specified\")\n",
    "        \n",
    "        if (missing(times )) { times  <- c(1:length(weights))       }\n",
    "        if (missing(sigmas)) { sigmas <- c(rep(1, length(weights))) }\n",
    "        \n",
    "        # standardization of data\n",
    "        #weights <- (weights - mean(weights))/sd(weights)\n",
    "        \n",
    "    }\n",
    "    N <- length(times)\n",
    "    \n",
    "    # compute data cells edges\n",
    "    cells_edges <- c(times[1], 0.5*(head(times,-1)+tail(times,-1)), tail(times,1))\n",
    "\n",
    "\n",
    "### Prior and Fitness function ###\n",
    "    if (data_mode==3) { # Points Measurements\n",
    "        # defining prior\n",
    "        if (missing(gamma)) {\n",
    "            #reported at the end of section 3.3 in Scargle(2013)\n",
    "            prior <- -(1.32 + 0.577 *log10(N)) \n",
    "        }\n",
    "        else { \n",
    "            prior <- log(gamma) \n",
    "        }\n",
    "        \n",
    "        # defining fitness\n",
    "        fitness <- function(b_k, a_k) { return (b_k**2 / (4*a_k)) }\n",
    "    }\n",
    "    else { # Event data | Binned data\n",
    "        # defining prior\n",
    "        if (missing(p1)) { \n",
    "            prior <- (log(gamma)) \n",
    "        }\n",
    "        else {\n",
    "            #equivalent to the one reported in Scargle(2013):  ncp_prior = log(73.53*p1*N**(-0.478))-4\n",
    "            #taken from code linked in Scargle(2013)\n",
    "            prior <- (log(p1 /(0.0136*N**(0.478))) - 4)\n",
    "        }\n",
    "        \n",
    "        # defining fitness\n",
    "        fitness <- function(N_k, T_k) { return (N_k*(log(N_k) - log(T_k))) }\n",
    "    }\n",
    "    \n",
    "\n",
    "### iterate over data cells: ### -------------------------------------------------------------------\n",
    "    best <- numeric(length(times))\n",
    "    last <- numeric(length(times))\n",
    "    \n",
    "    for (k in 1:N) {\n",
    "        \n",
    "        if (data_mode==3) {\n",
    "            b_k <- rev(cumsum( rev(-weights[1:k]/(sigmas[1:k]*sigmas[1:k])) ))   #sum(x_n / s_n^2)\n",
    "            a_k <- rev(cumsum( rev(0.5/(sigmas[1:k]*sigmas[1:k])) ))             #sum(1 / 2*s_n^2)\n",
    "            \n",
    "            ## compute fitness of all possible last blocks obtained by adding k-th data cell\n",
    "            fitness_k <- fitness(b_k, a_k) + prior \n",
    "        }\n",
    "        else {\n",
    "            N_k <- rev(cumsum( rev(weights[1:k]) ))     # counts of every possible last block\n",
    "            T_k <- cells_edges[k+1] - cells_edges[1:k]  # all the possible lengths of last block\n",
    "            \n",
    "            ## compute fitness of all possible last blocks obtained by adding k-th data cell\n",
    "            fitness_k <- fitness(N_k, T_k) + prior\n",
    "        }\n",
    "\n",
    "        ## compute all possible partitions\n",
    "        A_k <- fitness_k + c(0, best[1:k-1])\n",
    "        \n",
    "        ## store best overall fitness (best A(k)) and last change point\n",
    "        best[k] <- max(A_k)\n",
    "        last[k] <- which.max(A_k)\n",
    "    }\n",
    "### end iterations ### -----------------------------------------------------------------------------\n",
    "    \n",
    "    # retrieve change points positions from \"last\" vector\n",
    "    change_points <- c()\n",
    "    icp <- last[length(times)]\n",
    "    while (icp > 1) {\n",
    "        change_points <- c(icp, change_points)\n",
    "        icp <- last[icp-1]\n",
    "    }\n",
    "        \n",
    "    return (cells_edges[change_points])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ApplyChangePointsToHist <- function(cpts, data, times=0) {\n",
    "    #It computes the left bin edges and counts of 'data' using change points in 'cpts'\n",
    "    \n",
    "    if (missing(times)) { times <- c(1:length(data)) }\n",
    "    \n",
    "    edges <- c(times[1], cpts, tail(times, 1))\n",
    "    #bin_centers <- 0.5*(tail(edges, -1) + head(edges, -1))\n",
    "    \n",
    "    cells_edges <- c(times[1], 0.5*(head(times,-1)+tail(times,-1)), tail(times,1))\n",
    "    width <- diff(cells_edges)\n",
    "        \n",
    "    counts <- numeric(length(cpts)+1)\n",
    "    for (jdx in 1:length(counts)) {\n",
    "        Wcells <- 0 # denominator for mean computation\n",
    "        for (idx in 1:length(data)) {\n",
    "            if ((times[idx] < edges[jdx+1]) & (times[idx] >= edges[jdx])) {\n",
    "                counts[jdx] <- counts[jdx] + data[idx]*width[idx]\n",
    "                Wcells <- Wcells + width[idx]\n",
    "            }\n",
    "        }\n",
    "        counts[jdx] <- counts[jdx]/Wcells\n",
    "    }\n",
    "    return (list(bins=head(edges,-1), counts=counts))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the algorithm for different datasets:\n",
    "\n",
    "Almost any kind of physical variable and any measurement framework can be accomodated to be processed with this algorithm. With the implementation above we are ready to test the algorithm under the different assumptions made in [1].\n",
    "\n",
    "The three main examples stressed in [1] are:\n",
    "- Data mode 1: Time Tagged Events (TTE) data\n",
    "- Data mode 2: Binned data\n",
    "- Data mode 3: Point measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mode 1: Earthquakes in California\n",
    "\n",
    "This kind of data are the most raw ones and can be represented by series of times of discrete events. \n",
    "\n",
    "An example can be the cosmic rays detected with a scintillator and registered as a sequence of arrival times.\n",
    "\n",
    "The fitness function for a block of these data is the log likelihood: <br>\n",
    "<br>\n",
    "\n",
    "<center>$ \\log(L^{(k)}(\\lambda))=N^{(k)}\\log(\\lambda)-\\lambda T^{(k)}  \\quad \\longrightarrow \\quad \n",
    "\\log(L^{(k)}_{max})+N^{(k)}=N^{(k)}\\log(\\frac{N^{(k)}}{T^{(k)}}) \\quad ; \\quad (\\lambda = \\frac{N^{(k)}}{T^{(k)}})$</center> \n",
    "\n",
    "where:\n",
    "- $N^{(k)}$ is the number of events in block $k$ \n",
    "- $T^{(k)}$ is the length of the block $k$ \n",
    "- $\\lambda$ is the estimated costant value of the signal inside a block\n",
    "\n",
    "From simulations of signal-free observational noise, the results of extensive simulations for a range of values of N and the adopted *correct detection rate* $p_0$ were found in [2] to be well fit with the `prior`:\n",
    "<br><br>\n",
    "<center>$prior = 4 − \\log(73.53p_1N^{−0.478})$</center> \n",
    "\n",
    "where:\n",
    "- $p_1 = 1 - p_0$, is the *false positive rate*.\n",
    "\n",
    "We decided to test our algorithm for this kind of data with a dataset that contains informations about earthquakes in California in the years 1982-2011.\n",
    "We run our algorithm over the dataset filtered for uncorrelated events and try to see if the algorithm is able to detect efficiently changes in the event frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "data <- read.table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\", header=FALSE,\n",
    "                   col.names=c('index','trigger','time','magnitudes','X','Y','Z'))\n",
    "head(data)\n",
    "\n",
    "#filter by uncorrelated events\n",
    "timetags <- data$time[data$trigger==-1]\n",
    "\n",
    "cat(\"Length of the dataset:\", length(timetags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the algorithm\n",
    "cpts <- Bayesian_Blocks(data_mode=1, times=timetags, p1=0.01)\n",
    "cat(\"Number of Change Points:\", length(cpts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results \n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "# data rebinned with Bayesian Block algorithm\n",
    "edges <- c(timetags[1], cpts, tail(timetags, 1))\n",
    "\n",
    "h1 <- hist(timetags, breaks=length(edges)*12, plot=FALSE)\n",
    "h2 <- hist(timetags, breaks=edges, plot=FALSE)\n",
    "\n",
    "#plot the results\n",
    "plot(head(h1$breaks,-1), h1$density, col=\"blue\", type=\"s\", main=\"Earthquakes frequency distribution\",\n",
    "     xlab='Time [s]',ylab='Density',ylim=c(0,max(h2$density)))\n",
    "lines(head(h2$breaks,-1), h2$density, col=\"red\", type=\"s\", lwd=1.5)\n",
    "\n",
    "legend(\"topright\", inset=c(-0.1,0), legend=c(\"Evenly spaced bins\", \"Bayesian Blocks\"), col=c(\"blue\",\"red\"),\n",
    "      lwd=c(2,2), lty=c(1,1), border=FALSE, box.lty=0)\n",
    "grid()\n",
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard deviation of frequencies computed in B.B. bins\n",
    "freqs <- c()\n",
    "for (i in 1:(length(cpts)-1)){\n",
    "    a <- data[data$time>=cpts[i] & data$time<cpts[i+1] , ]\n",
    "    freqs <- c(freqs,length(unlist(a['time']))/(cpts[i+1]-cpts[i]))\n",
    "}\n",
    "\n",
    "cat(\"Standard deviation of frequency from Bayesian Blocks:\", sd(freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard deviation of frequencies computed in evenly spaced bins\n",
    "step <- max(data$time)/length(cpts)\n",
    "freqs <- c()\n",
    "for (i in 1:length(cpts)){\n",
    "    a <- data[data$time>=(i-1)*step & data$time<i*step , ]\n",
    "    freqs <- c(freqs,length(unlist(a['time']))/(step))\n",
    "}\n",
    "\n",
    "cat(\"Standard deviation of frequency from evenly spaced bins:\", sd(freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mode 2: Am-Cs-Co spectra\n",
    "\n",
    "This data are similar to the ones presented above, but with the events collected into bins, which do not have to be equal or evenly spaced. In other words, in this case we are already working with histograms.\n",
    "\n",
    "Constructing a histogram of non-sequential measured values is very similar to the estimation of a piecewise constant model for the same data treated as if they were sequential. Hence, histograms can be constructed by simply ordering the measured values and applying our algorithm for event data.\n",
    "\n",
    "The Likelihood for bin $n$ is given by the Poisson distribution:\n",
    "\n",
    "<center>$L_n = \\frac{(\\lambda e_nW_n)^{N_n}e^{-\\lambda e_nW_n}}{N_n!}  \\quad \\longrightarrow \\quad \n",
    "\\log(L^{(k)}(\\lambda))=N^{(k)}\\log(\\lambda)-\\lambda w^{(k)} $</center>\n",
    "\n",
    "where:\n",
    "- $W_n$ is the width of the bin\n",
    "- $e_n$ is the exposure factor of the bin n\n",
    "- $\\lambda$ is the true event rate at the detector\n",
    "- $w_n$ is the bin efficiency, $W_ne_n$\n",
    "\n",
    "We test our algorithm with data collected using a germanium detector from a source of $Am^{241}$, $Cs^{137}$ and $Co^{60}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "data <- scan(\"B19036_AmCsCo_20180316.dat\", skip=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the algorithm\n",
    "cpts <- Bayesian_Blocks(data_mode=2, weights=data, p1=0.01)\n",
    "\n",
    "cat(\"Found \", length(cpts), \" change points.\")\n",
    "\n",
    "#compute the counts\n",
    "hs <- ApplyChangePointsToHist(cpts=cpts, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "par(mfrow=c(1,1))\n",
    "options(repr.plot.width=12, repr.plot.height=8) #oppure 10,7 \n",
    "\n",
    "# original data\n",
    "tts <- 1:length(data)\n",
    "Norm <- sum(data+1)\n",
    "\n",
    "plot(tts, (data+1)/Norm, type=\"s\", col=rgb(0,0,1,alpha=0.6), log='y',\n",
    "    xlab=\"Energy [ADC channel]\", ylab=\"Density\", main=\"Am-Cs-Co spectra with B.B. algorithm\")\n",
    "\n",
    "polygon(tts, (data+1)/Norm, col = rgb(0,0,1,alpha=0.1), border=FALSE)\n",
    "\n",
    "# data rebinned with Bayesian Block algorithm\n",
    "lines(hs$bins, (hs$counts+1)/Norm, type=\"s\", col=rgb(1,0,0,alpha=1), lwd=1.5)\n",
    "\n",
    "legend(\"topright\", inset=c(-0.1,0), legend=c(\"Original data\", \"Bayesian Blocks\"), col=c(\"blue\",\"red\"),\n",
    "      lwd=c(2,2), lty=c(1,1), border=FALSE, box.lty=0)\n",
    "grid()\n",
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel density estimation\n",
    "tts <- 1:length(data)\n",
    "Norm <- sum(data+1)\n",
    "\n",
    "KDE <- density(tts, weights=(data+1)/Norm, bw=8)\n",
    "\n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "plot(tts, (data+1)/Norm, type=\"s\", col=rgb(0,0,1,alpha=0.6), log='y',\n",
    "    xlab=\"Energy [ADC channel]\", ylab=\"Density\", main=\"Am-Cs-Co spectra with KDE\")\n",
    "\n",
    "polygon(tts, (data+1)/Norm, col = rgb(0,0,1,alpha=0.1), border=FALSE)\n",
    "\n",
    "lines(KDE, type=\"s\", col=\"red\", lwd=2)\n",
    "\n",
    "legend(\"topright\", inset=c(-0.1,0), legend=c(\"Original data\", \"Kernel Density Est.\"), col=c(\"blue\",\"red\"),\n",
    "      lwd=c(2,2), lty=c(1,1), border=FALSE, box.lty=0)\n",
    "grid()\n",
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration\n",
    "kevs <- c(59.54, 661.66, 1173.24, 1332.51) # energy values in keV\n",
    "ADCs <- c(which.max(data[1:1000]), which.max(data[1000:2000])+1000, \n",
    "          which.max(data[2500:3200])+2500, which.max(data[3201:4000])+3201)\n",
    "\n",
    "calib <- lm(kevs ~ I(ADCs))\n",
    "xx <- c(seq(min(ADCs), max(ADCs), len=500))\n",
    "yy <- calib$coefficients[1] + calib$coefficients[2]*xx\n",
    "\n",
    "# plot \n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "plot(ADCs, kevs, pch=19, col=\"blue\", main=\"Spectrum Calibration\", \n",
    "     xlab=\"Energy [ADC channel]\", ylab=\"Energy [keV]\")\n",
    "lines(xx, yy, type=\"l\", col=\"red\")\n",
    "\n",
    "label <- paste0(\"f(x) = a*x + b\\n    a : \", formatC(calib$coefficients[[2]], width=5), \n",
    "                \"\\n    b : \", formatC(calib$coefficients[[1]], width=5))\n",
    "text(500, 1000, labels=label,  pos=4, cex=1.2, xpd=TRUE)\n",
    "grid()\n",
    "\n",
    "# residuals plot\n",
    "par(new=TRUE, omi=c(0.8,0,0,0.5))\n",
    "layout(matrix(1:4,2))\n",
    "\n",
    "plot(ADCs, calib$residuals, type=\"p\", col=\"blue\", pch=18, cex=1.2, main=\"Residuals\",\n",
    "     xlab=\"\", ylab=\"\")\n",
    "lines(ADCs, c(rep(0,length(ADCs))), type=\"l\", lty=2, col=\"red\")\n",
    "segments(x0=ADCs,y0=rep(0,length(ADCs)), y1=calib$residuals, col=rgb(0,0,1,alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation\n",
    "\n",
    "To show how the algorithm works we decide to produce an animation. To produce the frames, we run the algorithm in a cycle over the entire dataset passing to it the first `i` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  <- scan(\"B19036_AmCsCo_20180316.dat\", skip=2)\n",
    "times <- c(1:length(data))\n",
    "Norm <- sum(data+1)\n",
    "\n",
    "#calibration\n",
    "times <- calib$coefficients[1] + calib$coefficients[2]*times\n",
    "\n",
    "steps <- c(seq(1, 8192, 80), 8192)\n",
    "\n",
    "# loop\n",
    "system(\"mkdir frames\")\n",
    "for (i in steps) {\n",
    "    cps <- Bayesian_Blocks(data_mode=2,times=times[1:i], weights=data[1:i], p1=0.01)\n",
    "    counts <- ApplyChangePointsToHist(cps, data[1:i], times=times[1:i])\n",
    "    \n",
    "    name <- paste0(\"frames/frame_\", formatC(i, width=4, flag=\"0\"), \".png\")\n",
    "    png(file=name, width=1280, heigh=720)\n",
    "    \n",
    "    # original data\n",
    "    plot(times[1:i], (data[1:i]+1)/Norm, type=\"s\", col=rgb(0,0,1,alpha=0.6), log=\"y\", \n",
    "         ylim=c(1,max(data))/Norm, xlim=c(1,max(times)), xlab=\"Energy [keV]\", ylab=\"Density\", \n",
    "         main=\"Am−Cs−Co spectra with B.B. algorithm\")\n",
    "\n",
    "    polygon(times[1:i], c(head(data[1:i]+1,-1), 1)/Norm, col = rgb(0,0,1,alpha=0.1), border=FALSE)\n",
    "\n",
    "    # data rebinned with Bayesian Block algorithm\n",
    "    lines(counts$bins, (counts$counts+1)/Norm, type=\"s\", col=rgb(1,0,0,alpha=1), lwd=2)\n",
    "    \n",
    "    x1 <- tail(counts$bins, 1)          ; x2 <- times[i]\n",
    "    y1 <- tail(counts$counts+1, 1)/Norm ; y2 <- 1/Norm\n",
    "    polygon(c(x1,x1,x2,x2), c(y2,y1,y1,y2), col=rgb(1,0,0,alpha=0.4), border=FALSE)\n",
    "\n",
    "    legend(\"topright\", legend=c(\"Original data\", \"Bayesian Blocks\"), col=c(\"blue\",\"red\"),\n",
    "          lwd=c(2,2), lty=c(1,1), box.lty=0) #inset=c(-0.1,0.01)\n",
    "    grid()\n",
    "    box()\n",
    "    \n",
    "    dev.off()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"convert -delay 10 frames/*.png BayesianBlocksAnimation.gif\")\n",
    "\n",
    "library(\"IRdisplay\")\n",
    "display_png(file=\"BayesianBlocksAnimation.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mode 3: Point Measurement simulation\n",
    "\n",
    "These data represent the measurements of a physical quantity during time. We want to characterize its time dependence.  Inevitable corruption due to observational errors is frequently countered by smoothing the data and/or fitting a model. As with the other data modes Bayesian Blocks is a different approach to this issue, making use of knowledge of the observational error distribution and avoiding the information loss entailed by smoothing.\n",
    "\n",
    "Considering the case where the errors are taken to obey a normal probability distribution with zero mean and given variance, a theorical treatment gives the fitness function at the first order:\n",
    "\n",
    "<center> $\\log(L^{(k)}_{max})=\\frac{b_k^{2}}{4a_k}$ </center>\n",
    "\n",
    "where:\n",
    "- $a_k = \\frac{1}{2}\\sum_{n}{\\frac{1}{\\sigma_n^2}}$\n",
    "<br><br>\n",
    "- $b_k = -\\sum_n{\\frac{x_n}{\\sigma_n^2}}$\n",
    "\n",
    "A simulation study reported in [1] to calibrate the `prior` for normally distributed point measurements depicts the relation:\n",
    "<br><br>\n",
    "<center>$prior = 1.32 + 0.577\\log_{10}(N)$</center>\n",
    "\n",
    "where $N$ is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1230642885)\n",
    "\n",
    "mexican_hat <- function(t, sigma, pos=0) {     # taken from Wikipedia\n",
    "    return ( (2/(sqrt(3*sigma)*pi**(0.25)))*(1 -(t-pos)**2/sigma**2)*exp(-(t-pos)**2 / (2*sigma**2)) )\n",
    "}\n",
    "\n",
    "N <- 1000\n",
    "amplitude <- runif(5,10,100)\n",
    "sigma <- runif(5,0,100)\n",
    "position <- runif(5,0,1000)\n",
    "t <- 1:N\n",
    "\n",
    "# building signal\n",
    "signal <- numeric(N)\n",
    "for (jj in 1:5) {\n",
    "    signal <- signal + amplitude[jj]*mexican_hat(t, sigma[jj], position[jj])\n",
    "}\n",
    "noise  <- rnorm(N, mean = 0, sd = 1)\n",
    "simul <- signal + noise\n",
    "\n",
    "cpts <- Bayesian_Blocks(data_mode=3, times=t, weights=simul, sigmas=c(rep(1,N)))\n",
    "hs   <- ApplyChangePointsToHist(cpts, simul, t)\n",
    "\n",
    "# plot\n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "# original data\n",
    "plot(t, simul, type=\"l\", col=rgb(0,0,1,alpha=0.6), xlab=\"Time [a.u.]\", ylab=\"Amplitude [a.u.]\", \n",
    "     main=\"B.B. points measurements simulation\")\n",
    "\n",
    "#polygon(t, simul, col = rgb(0,0,1,alpha=0.1), border=FALSE)\n",
    "\n",
    "# data rebinned with Bayesian Block algorithm\n",
    "lines(hs$bins, hs$counts, type=\"s\", col=rgb(1,0,0,alpha=1), lwd=1.5)\n",
    "\n",
    "lines(t, signal, type=\"s\", col= rgb(0,1,0,alpha=1), lwd=2)\n",
    "\n",
    "legend(\"topleft\", inset=c(0,0.01), legend=c(\"Original data\", \"Bayesian Blocks\",'True signal'), \n",
    "       col=c(\"blue\",\"red\",'green'),\n",
    "      lwd=c(2,2), lty=c(1,1), border=FALSE, box.lty=0)\n",
    "grid()\n",
    "box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Analysis:\n",
    "The number of possible partitions (i.e., the number of ways $N$ cells can be arranged in blocks) is $2^{N-1}$. \n",
    "The algorithm implemented should follow a computational time scaling of order $O(N^2)$, performing implicitly a complete search of this space. \n",
    "Indeed, the algorithm is able to find the global optimum among all partitions without an exhaustive explicit search, which is obviously impossible for almost any value of $N$ arising in practice.\n",
    "\n",
    "In the following we test the computational time of the algorithm and fit the resulting plot with a quadratic curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  <- scan(\"B19036_AmCsCo_20180316.dat\", skip=2)\n",
    "times <- c(1:length(data))\n",
    "\n",
    "steps <- c(seq(1, length(data), 500), length(data))\n",
    "comp_times <- c()\n",
    "for (i in steps) {\n",
    "    start_time <- Sys.time()\n",
    "    cps <- Bayesian_Blocks(data_mode=2,times=times[1:i], weights=data[1:i], p1=0.01)\n",
    "    end_time <- Sys.time()\n",
    "    comp_times <- c(c(comp_times),as.numeric(end_time - start_time))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x <- steps\n",
    "y <- comp_times\n",
    "\n",
    "# second order fit\n",
    "fit <- lm(y ~ I(x)+I(x^2))\n",
    "xx <- steps\n",
    "yy <- predict(fit,data.frame(x=xx))\n",
    "\n",
    "# plot\n",
    "options(repr.plot.width=12, repr.plot.height=8)\n",
    "\n",
    "# measures of time\n",
    "plot(steps, comp_times, type=\"p\", col=rgb(0,0,1,alpha=0.5), xlab=\"N \", ylab=\"Time [s]\", \n",
    "     main=\"Computational Time as function of N\", pch=18, cex=1.5)\n",
    "\n",
    "# fitted line\n",
    "lines(xx,yy, type=\"l\", col=\"red\", lwd=1.5)\n",
    "\n",
    "legend(\"right\", inset=c(-0.1,0), legend=c(\"Measurements\", \"Fitted model\"), col=c(\"blue\",\"red\"),\n",
    "      lwd=c(2,2), lty=c(0,1), pch=c(18,NA), border=FALSE, box.lty=0)\n",
    "\n",
    "label <- paste0(\"f(x) = a*x² + b*x + c\\n    a : \", formatC(fit$coefficients[[3]], width=5), \n",
    "                \"\\n    b : \", formatC(fit$coefficients[[2]], width=5),\n",
    "                \"\\n    c : \", formatC(fit$coefficients[[1]], width=5),\n",
    "                \"\\n    chi² : \", formatC(sum(fit$residuals**2/yy), width=5) )\n",
    "text(6700, 0.5, labels=label,  pos=4, cex=1.2, xpd=TRUE)\n",
    "\n",
    "grid()\n",
    "box()\n",
    "\n",
    "# residuals plot\n",
    "par(new=TRUE, omi=c(0,1,0.8,0))\n",
    "layout(matrix(4:1,2))\n",
    "\n",
    "plot(steps, fit$residuals, type=\"p\", col=\"blue\", pch=18, cex=1.2, main=\"Residuals\",\n",
    "     xlab=\"\", ylab=\"\", bg=\"white\")\n",
    "lines(steps, c(rep(0,length(steps))), type=\"l\", lty=2, col=\"red\")\n",
    "segments(x0=steps,y0=rep(0,length(steps)), y1=fit$residuals, col=rgb(0,0,1,alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [1] J. D. Scargle et al., Astrophys. J. 764 (2013) 167, URL: https://iopscience.iop.org/article/10.1088/0004-637X/764/2/167\n",
    "- [2] J. D. Scargle et al., *The Bayesian Block Algorithm*, 2013, URL: https://arxiv.org/abs/1304.2818\n",
    "- [3] J. D. Scargle et al., Astrophys. J. 504 (1998) 405, URL: https://arxiv.org/abs/astro-ph/9711233"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
